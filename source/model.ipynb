{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b038e62",
   "metadata": {},
   "source": [
    "### Github에서 source를 내려받고 directory를 이동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb53da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/scpark20/AI-for-Music.git\n",
    "import os\n",
    "os.chdir('AI-for-Music/source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adcd067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e38f1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "from hparams import dataset_hparams\n",
    "\n",
    "model_hparams = EasyDict(n_tokens = dataset_hparams.offsets.pedal_off,\n",
    "                         embedding_dim = 512,\n",
    "                         hidden_dim = 1024\n",
    "                        )      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd15d51e",
   "metadata": {},
   "source": [
    "### Model을 구성합니다.\n",
    "Model은 Embedding, RNN (LSTM), Linear Layer로 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a60f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, model_hparams):\n",
    "        super().__init__()\n",
    "        self.hp = model_hparams\n",
    "        self.step = nn.Parameter(torch.zeros(1).long(), requires_grad=False)\n",
    "        \n",
    "        # Embdding Layer\n",
    "        # token값을 vector로 변환합니다. \n",
    "        self.embedding = nn.Embedding(self.hp.n_tokens, self.hp.embedding_dim)\n",
    "        \n",
    "        # RNN Layer\n",
    "        # 현재의 input vector와 이전 시점의 state vector를 입력 받아 현재 시점의 state vector를 출력합니다.\n",
    "        self.rnn = nn.LSTM(input_size=self.hp.embedding_dim, hidden_size=self.hp.hidden_dim,\n",
    "                        num_layers=3, batch_first=True, dropout=0.1)\n",
    "        \n",
    "        # Layer Layer\n",
    "        # 현재 시점의 state vector를 입력 받아 미래 시점의 token에 대한 probability distribution을 출력합니다.\n",
    "        self.out_layer = nn.Linear(self.hp.hidden_dim, self.hp.n_tokens)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x : (batch, length)\n",
    "        \n",
    "        # (batch, length, model_dim)\n",
    "        x = self.embedding(x)\n",
    "        # (batch, length, hidden_dim)\n",
    "        x, _ = self.rnn(x)\n",
    "        # (batch, length, n_tokens)\n",
    "        x = self.out_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def _get_initial_state(self, batch_size):\n",
    "        h = torch.zeros(3, batch_size, self.hp.hidden_dim)\n",
    "        c = torch.zeros(3, batch_size, self.hp.hidden_dim)\n",
    "        return (h, c)\n",
    "    \n",
    "    def inference(self, x, state=None, temperature=1.0):\n",
    "        # x : (batch, length)\n",
    "        \n",
    "        # (batch, length, model_dim)\n",
    "        x = self.embedding(x)\n",
    "        # (batch, length, hidden_dim)\n",
    "        x, state = self.rnn(x, state)\n",
    "        # (batch, length, n_tokens)\n",
    "        x = self.out_layer(x)\n",
    "        # (batch, 1)\n",
    "        x = torch.distributions.categorical.Categorical(logits=x[:, -1:]/temperature).sample()\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26050b5f",
   "metadata": {},
   "source": [
    "### Model Init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a019ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embedding): Embedding(389, 512)\n",
      "  (rnn): LSTM(512, 1024, num_layers=3, batch_first=True, dropout=0.1)\n",
      "  (out_layer): Linear(in_features=1024, out_features=389, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model(model_hparams)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7bb008",
   "metadata": {},
   "source": [
    "### Test for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0165f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 389])\n"
     ]
    }
   ],
   "source": [
    "batch = 2\n",
    "length = 100\n",
    "x = torch.randint(389, size=(batch, length))\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef4840",
   "metadata": {},
   "source": [
    "### Test for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d20e1a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n",
      "torch.Size([3, 2, 1024]) torch.Size([3, 2, 1024])\n"
     ]
    }
   ],
   "source": [
    "state = model._get_initial_state(batch)\n",
    "x = torch.randint(389, size=(batch, 1))\n",
    "y, (h, c) = model.inference(x, state)\n",
    "print(y.shape)\n",
    "print(h.shape, c.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
