{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920b9507",
   "metadata": {},
   "source": [
    "### Preprocess한 Maestro dataset .npy 파일들을 다운로드 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41e2c083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (3.13.0)\n",
      "Requirement already satisfied: requests[socks]>=2.12.0 in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from gdown) (2.26.0)\n",
      "Requirement already satisfied: filelock in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from gdown) (3.0.12)\n",
      "Requirement already satisfied: tqdm in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from gdown) (4.60.0)\n",
      "Requirement already satisfied: six in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (2.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (1.7.1)\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1PCsy4C0XdO2hWedw7w6FZ39neCgAdNAi\n",
      "To: /home/gaudio/ste/projects/lectures/AI-for-Music/source/maestro-npy.zip\n",
      "100%|████████████████████████████████████████| 124M/124M [00:32<00:00, 3.81MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Google drive에서 받기 위해 gdown을 설치합니다\n",
    "!pip install gdown\n",
    "# maestro-npy.zip 파일을 다운로드 합니다.\n",
    "!gdown https://drive.google.com/uc?id=1PCsy4C0XdO2hWedw7w6FZ39neCgAdNAi\n",
    "# maestro-npy.zip 압축파일을 풉니다.\n",
    "!unzip -o -qq maestro-npy.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4694a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a8ee7",
   "metadata": {},
   "source": [
    "### Dataset에 관련된 hyper-parameters를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf3bea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir dataset/\n",
      "max_note_duration 2\n",
      "token_length 1024\n",
      "dims {'interval': 100, 'velocity': 32, 'note_on': 128, 'note_off': 128, 'pedal_on': 1, 'pedal_off': 1}\n",
      "offsets {'interval': 100, 'velocity': 100, 'note_on': 132, 'note_off': 260, 'pedal_on': 388, 'pedal_off': 389}\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "# 각 이벤트 종류 (interval, velocity 등)에 대해 사용할 token의 갯수를 지정합니다.\n",
    "dims = EasyDict(interval = 100,\n",
    "                velocity = 32,\n",
    "                note_on = 128,\n",
    "                note_off = 128,\n",
    "                pedal_on = 1,\n",
    "                pedal_off = 1)\n",
    "\n",
    "# 각 이벤트 종류 (interval, velocity 등)에 대해 token의 offset을 지정합니다.\n",
    "offsets = EasyDict(interval = 100,\n",
    "                   velocity = dims.interval,\n",
    "                   note_on = dims.interval + dims.velocity,\n",
    "                   note_off = dims.interval + dims.velocity + dims.note_on,\n",
    "                   pedal_on = dims.interval + dims.velocity + dims.note_on + dims.note_off,\n",
    "                   pedal_off = dims.interval + dims.velocity + dims.note_on + dims.note_off + dims.pedal_on)\n",
    "\n",
    "# Dataset에 사용될 hyper-parameters를 지정합니다.\n",
    "dataset_hparams = EasyDict(root_dir = 'dataset/',\n",
    "                           max_note_duration = 2, # seconds\n",
    "                           token_length = 1024,\n",
    "                           dims = dims,\n",
    "                           offsets = offsets\n",
    "                          )\n",
    "for hp in dataset_hparams:\n",
    "    print(hp, dataset_hparams[hp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386b175",
   "metadata": {},
   "source": [
    "### Pytorch framework에서 사용하는 규격대로 Dataset class를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "537afea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from midi_utils import event_list_to_tokens\n",
    "\n",
    "class MaestroDataset(Dataset):\n",
    "    def __init__(self, dataset_hparams):\n",
    "        super().__init__()\n",
    "        self.hp = dataset_hparams\n",
    "        # .npy 파일의 목록을 저장합니다.\n",
    "        self.files = [os.path.join(self.hp.root_dir, file) for file in os.listdir(self.hp.root_dir) if 'npy' in file]\n",
    "    \n",
    "    # 전체 data 갯수를 반환합니다.\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    # 하나의 data index를 받아 token sequence를 반환합니다.\n",
    "    def __getitem__(self, index):\n",
    "        # file 경로를 구합니다.\n",
    "        file = self.files[index]\n",
    "        \n",
    "        # .npy 파일을 읽어 event_list를 만듭니다.\n",
    "        event_list = np.load(file, allow_pickle=True)\n",
    "        \n",
    "        # event_list를 token sequence로 변환합니다.\n",
    "        tokens = event_list_to_tokens(event_list, self.hp)\n",
    "        \n",
    "        # dataset에 사용될 만큼 token sequence를 자릅니다.\n",
    "        if len(tokens) < self.hp.token_length:\n",
    "            start_index = 0\n",
    "        else:\n",
    "            start_index = np.random.randint(0, len(tokens)-self.hp.token_length)\n",
    "        tokens = np.array(tokens[start_index:start_index+self.hp.token_length])\n",
    "        \n",
    "        # token sequence가 dataset에 사용될 길이보다 짧은 경우 padding을 합니다.\n",
    "        tokens_padded = np.zeros(self.hp.token_length, dtype=np.int)\n",
    "        tokens_padded[:len(tokens)] = tokens\n",
    "        \n",
    "        # padding된 token sequence를 반환합니다.\n",
    "        return np.array(tokens_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8543f8b",
   "metadata": {},
   "source": [
    "### Pytorch에서 사용할 수 있도록 dataset과 train_loader를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25827fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MaestroDataset object at 0x7fd33e7a1e20>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7fd33e7a1a30>\n"
     ]
    }
   ],
   "source": [
    "dataset = MaestroDataset(dataset_hparams)\n",
    "print(dataset)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=16)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19b96729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024])\n",
      "0번 token sequence : [205   0 114 ...   1 333   0]\n",
      "1번 token sequence : [  1 327   0 ...   1 388   0]\n",
      "2번 token sequence : [ 22 342   0 ... 109 188   5]\n",
      "3번 token sequence : [325   1 121 ...   4 344   0]\n",
      "4번 token sequence : [  0 297   1 ... 311   1 323]\n",
      "5번 token sequence : [201   2 329 ... 196   0 389]\n",
      "6번 token sequence : [115 218   1 ... 117 221   0]\n",
      "7번 token sequence : [196   1 321 ... 111 217  16]\n",
      "8번 token sequence : [189   1 329 ...   0 109 184]\n",
      "9번 token sequence : [  0 388   1 ... 189   0 320]\n",
      "10번 token sequence : [117 204   1 ... 122 182   0]\n",
      "11번 token sequence : [192   2 320 ...   0 337  13]\n",
      "12번 token sequence : [233   1 360 ...   4 110 201]\n",
      "13번 token sequence : [  0 117 207 ...   0 114 179]\n",
      "14번 token sequence : [  0 324   0 ... 184   0 327]\n",
      "15번 token sequence : [172   0 115 ...   0 329   9]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.shape)\n",
    "    for i, data in enumerate(batch):\n",
    "        print(str(i) + '번 token sequence :', data.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0a5688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172, 0, 115, 184, 1, 111, 194, 0, 312, 0, 328, 0, 300, 1, 322, 1, 111, 192, 1, 320, ...\n"
     ]
    }
   ],
   "source": [
    "print(*data.data.cpu().numpy()[:20], '...', sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0c98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
