{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920b9507",
   "metadata": {},
   "source": [
    "### Preprocess한 Maestro dataset .npy 파일들을 다운로드 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41e2c083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (3.13.0)\n",
      "Requirement already satisfied: requests[socks]>=2.12.0 in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from gdown) (2.26.0)\n",
      "Requirement already satisfied: six in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied: filelock in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from gdown) (3.0.12)\n",
      "Requirement already satisfied: tqdm in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from gdown) (4.60.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (3.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/gaudio/anaconda3/envs/ste/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (1.7.1)\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1PCsy4C0XdO2hWedw7w6FZ39neCgAdNAi\n",
      "To: /home/gaudio/ste/projects/lectures/AI-for-Music/maestro-npy.zip\n",
      "100%|████████████████████████████████████████| 124M/124M [00:28<00:00, 4.29MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Google drive에서 받기 위해 gdown을 설치합니다\n",
    "!pip install gdown\n",
    "# maestro-npy.zip 파일을 다운로드 합니다.\n",
    "!gdown https://drive.google.com/uc?id=1PCsy4C0XdO2hWedw7w6FZ39neCgAdNAi\n",
    "# maestro-npy.zip 압축파일을 풉니다.\n",
    "!unzip -o -qq maestro-npy.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4694a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a8ee7",
   "metadata": {},
   "source": [
    "### Dataset에 관련된 hyper-parameters를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf3bea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir dataset/\n",
      "max_note_duration 2\n",
      "token_length 1024\n",
      "dims {'interval': 100, 'velocity': 32, 'note_on': 128, 'note_off': 128, 'pedal_on': 1, 'pedal_off': 1}\n",
      "offsets {'interval': 100, 'velocity': 100, 'note_on': 132, 'note_off': 260, 'pedal_on': 388, 'pedal_off': 389}\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "# 각 이벤트 종류 (interval, velocity 등)에 대해 사용할 token의 갯수를 지정합니다.\n",
    "dims = EasyDict(interval = 100,\n",
    "                velocity = 32,\n",
    "                note_on = 128,\n",
    "                note_off = 128,\n",
    "                pedal_on = 1,\n",
    "                pedal_off = 1)\n",
    "\n",
    "# 각 이벤트 종류 (interval, velocity 등)에 대해 token의 offset을 지정합니다.\n",
    "offsets = EasyDict(interval = 100,\n",
    "                   velocity = dims.interval,\n",
    "                   note_on = dims.interval + dims.velocity,\n",
    "                   note_off = dims.interval + dims.velocity + dims.note_on,\n",
    "                   pedal_on = dims.interval + dims.velocity + dims.note_on + dims.note_off,\n",
    "                   pedal_off = dims.interval + dims.velocity + dims.note_on + dims.note_off + dims.pedal_on)\n",
    "\n",
    "# Dataset에 사용될 hyper-parameters를 지정합니다.\n",
    "dataset_hparams = EasyDict(root_dir = 'dataset/',\n",
    "                           max_note_duration = 2, # seconds\n",
    "                           token_length = 1024,\n",
    "                           dims = dims,\n",
    "                           offsets = offsets\n",
    "                          )\n",
    "for hp in dataset_hparams:\n",
    "    print(hp, dataset_hparams[hp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1cd0eb",
   "metadata": {},
   "source": [
    "### event_list를 token sequence로 변환하는 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb703e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_list_to_tokens(event_list, hp):\n",
    "    # token들을 모아줄 list를 만듭니다.\n",
    "    tokens = []\n",
    "    prev_time = 0\n",
    "    \n",
    "    # event_list에서 event들을 하나씩 꺼내옵니다.\n",
    "    for event in event_list:\n",
    "        \n",
    "        # interval은 현재 event['time']에서 prev_time을 빼서 구합니다.\n",
    "        interval = event['time'] - prev_time\n",
    "        \n",
    "        # interval(시간)을 interval_token(자연수) 값으로 변환합니다.\n",
    "        interval_token = int(interval / hp.max_note_duration * hp.dims.interval)\n",
    "        interval_token = min(interval_token, hp.dims.interval)\n",
    "        \n",
    "        # tokens에 interval_token을 저장합니다.\n",
    "        tokens.append(interval_token)\n",
    "        \n",
    "        # 현재 event['time']을 prev_time으로 저장합니다.\n",
    "        prev_time = event['time']\n",
    "        \n",
    "        # 이벤트 'note_on'의 경우 velocity token과 note_on token을 저장합니다.\n",
    "        if event['type'] == 'note_on':\n",
    "            tokens.append(hp.offsets.velocity + int(event['velocity'] / 128 * hp.dims.velocity))\n",
    "            tokens.append(hp.offsets.note_on + event['note'])\n",
    "            \n",
    "        # 이벤트 'note_off'에 대한 token을 저장합니다.\n",
    "        elif event['type'] == 'note_off':\n",
    "            tokens.append(hp.offsets.note_off + event['note'])\n",
    "            \n",
    "        # 이벤트 'pedal_on'에 대한 token을 저장합니다.\n",
    "        elif event['type'] == 'pedal_on':\n",
    "            tokens.append(hp.offsets.pedal_on)\n",
    "            \n",
    "        # 이벤트 'pedal_off'에 대한 token을 저장합니다.\n",
    "        elif event['type'] == 'pedal_off':\n",
    "            tokens.append(hp.offsets.pedal_off)\n",
    "            \n",
    "    # token sequence를 반환합니다.\n",
    "    return np.array(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386b175",
   "metadata": {},
   "source": [
    "### Pytorch framework에서 사용하는 규격대로 Dataset class를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "537afea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaestroDataset(Dataset):\n",
    "    def __init__(self, dataset_hparams):\n",
    "        super().__init__()\n",
    "        self.hp = dataset_hparams\n",
    "        # .npy 파일의 목록을 저장합니다.\n",
    "        self.files = [os.path.join(self.hp.root_dir, file) for file in os.listdir(self.hp.root_dir) if 'npy' in file]\n",
    "    \n",
    "    # 전체 data 갯수를 반환합니다.\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    # 하나의 data index를 받아 token sequence를 반환합니다.\n",
    "    def __getitem__(self, index):\n",
    "        # file 경로를 구합니다.\n",
    "        file = self.files[index]\n",
    "        \n",
    "        # .npy 파일을 읽어 event_list를 만듭니다.\n",
    "        event_list = np.load(file, allow_pickle=True)\n",
    "        \n",
    "        # event_list를 token sequence로 변환합니다.\n",
    "        tokens = event_list_to_tokens(event_list, self.hp)\n",
    "        \n",
    "        # dataset에 사용될 만큼 token sequence를 자릅니다.\n",
    "        if len(tokens) < self.hp.token_length:\n",
    "            start_index = 0\n",
    "        else:\n",
    "            start_index = np.random.randint(0, len(tokens)-self.hp.token_length)\n",
    "        tokens = np.array(tokens[start_index:start_index+self.hp.token_length])\n",
    "        \n",
    "        # token sequence가 dataset에 사용될 길이보다 짧은 경우 padding을 합니다.\n",
    "        tokens_padded = np.zeros(self.hp.token_length, dtype=np.int)\n",
    "        tokens_padded[:len(tokens)] = tokens\n",
    "        \n",
    "        # padding된 token sequence를 반환합니다.\n",
    "        return np.array(tokens_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8543f8b",
   "metadata": {},
   "source": [
    "### Pytorch에서 사용할 수 있도록 dataset과 train_loader를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25827fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MaestroDataset object at 0x7f97c1589d00>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f957ad572e0>\n"
     ]
    }
   ],
   "source": [
    "dataset = MaestroDataset(dataset_hparams)\n",
    "print(dataset)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=16)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19b96729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024])\n",
      "0번 token sequence : [  0 111 175 ... 208   2 336]\n",
      "1번 token sequence : [119 202   1 ...   1 321   0]\n",
      "2번 token sequence : [118 194   3 ...   0 118 200]\n",
      "3번 token sequence : [308   1 349 ...   0 117 182]\n",
      "4번 token sequence : [  0 336   0 ...   0 124 187]\n",
      "5번 token sequence : [  0 117 216 ...   1 120 222]\n",
      "6번 token sequence : [114 191   0 ...   0 116 185]\n",
      "7번 token sequence : [179   5 307 ... 343   7 323]\n",
      "8번 token sequence : [  0 339   3 ... 121 219   0]\n",
      "9번 token sequence : [116 206   0 ... 200   0 123]\n",
      "10번 token sequence : [194   3 322 ...   3 328   3]\n",
      "11번 token sequence : [117 201   2 ...   6 117 197]\n",
      "12번 token sequence : [184   1 324 ...   2 119 204]\n",
      "13번 token sequence : [338   1 331 ... 388   1 312]\n",
      "14번 token sequence : [197   1 322 ... 117 198   0]\n",
      "15번 token sequence : [115 204   0 ... 116 200   3]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.shape)\n",
    "    for i, data in enumerate(batch):\n",
    "        print(str(i) + '번 token sequence :', data.numpy())\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
